<!doctype html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Google Dork</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="res/styles3.css" type="text/css" />
</head>

<body>
  <div class='page'>
    <h1 class='title'>Google Dork</h1><br />Several years ago, Johnny Long created a list of useful Google searches to
    find vulnerable systems. He calledmeach individual search a Google Dork, and the entire inventory of all these
    searches is known as the Google Hacking Database (GHDB).<br />Today, Exploit-DB took over where Johnny left off and
    make a searchable list of the updated GHDB available online at <a
      href="https://www.exploit-db.com/google-hacking-database" target="_blank">https://www.exploit-db.com/google-hacking-database</a><br /><br /><strong>
      <h3>Categories of the Google Hacking Database(GHDB) of exploit-db</h3>
    </strong><br />• <strong>Footholds:</strong> A lot of these searches find admin login pages for various common
    web-based software environments.<br />• <strong>Files Containing Usernames</strong><br />• <strong>Sensitive
      Directories</strong><br />• <strong>Web Server Detection</strong><br />• <strong>Vulnerable Files</strong><br />•
    <strong>Vulnerable Servers:</strong> web servers that may have a vulnerability, a category similar to Advisories and
    vulnerabilities.<br />• <strong>Error Messages</strong><br />• <strong>Files Containing Juicy Info:</strong> files
    that are often associated with caches and logging. Although they don&#39;t look for passwords directly, this cache
    and log information could be useful in learning more about the target organization.<br />• <strong>Files Containing
      Passwords</strong>: Numerous tools generate files that contain either cleartext passwords or encrypted/hashed
    passwords. These searches identify when such files are available via a web server.<br />• <strong>Sensitive Online
      Shopping Info</strong>: pages that may reveal sensitive online shopping info, including customer orders, weak
    shopping cart implementations, and overly detailed product information.<br />• <strong>Network or Vulnerability
      Data</strong>: pages that hold logs and/or configuration information about network devices, such as firewalls,
    VPNs, routers, Intrusion Detection Systems, and so on.<br />• <strong>Pages Containing Login Portals</strong><br />•
    <strong>Various Online Devices:</strong> locate web-based video cameras, printers, and various kinds of
    appliances.<br />• <strong>Advisories and Vulnerabilities:</strong> vulnerable systems, usually by identifying a
    known flawed CGI script using the inurl directive or a page with known flaws identified with the intitle
    directive.<br /><br /><strong><span style="color:#a5452a;">Examples</span></strong><br />
    <div class="codebox">
      <div class="codebox">
        intitle:&quot;index&nbsp;of&quot;&nbsp;intext:&quot;secring.skr&quot;|&quot;secring.pgp&quot;|&quot;secring.bak&quot;
      </div>
    </div><br /> secring.skr, secring.pgp, or secring.bak are the common filenames where private keys are stored for the
    Pretty Good Privacy (PGP) and Gnu Privacy Guard (GnuPG) tools<br />
    <div class="codebox">
      <div class="codebox">site:example.com&nbsp;intitle:&quot;index&nbsp;of&quot;&nbsp;bash_history</div>
    </div><br /> bash file history commands, showing the commands typed into the shell of a website<br />
    <div class="codebox">
      <div class="codebox">robots.txt&nbsp;disallow&nbsp;filetype:txt</div>
    </div><br /> A website administrator can use a robots.txt file to tell well-behaved search engine crawlers to ignore
    certain directories or pages on the website, with the Disallow syntax in this file. That way, those pages or
    directories won&#39;t be searchable or cached by the search engines. But robots.txt is a double-edged sword,
    indicating where sensitive items may be stored.<br />
    <div class="codebox">
      <div class="codebox">
        intitle:&quot;Nessus&nbsp;Scan&nbsp;Report&quot;&nbsp;&quot;This&nbsp;file&nbsp;was&nbsp;generated&nbsp;by&nbsp;Nessus&quot;
      </div>
    </div><br /> Find sites that have output files from the Nessus vulnerability scanner. Somehow, this file was placed
    in a directory under the document root of a web server so that Google could find it and make it searchable. A list
    of vulnerabilities of a target site could be useful for an attacker. We can save time running a vulnerability
    scanner by analyzing the results of earlier scans already done by the target organization<br /> <br /><strong>
      <h3>To know more:</h3>
    </strong><br />• <strong>Books</strong><br /> ◇ Google Hacking for Penetration Testers(Johnny Long): <a
      href="https://www.amazon.it/Google-Hacking-Penetration-Testers-Johnny/dp/0128029641" target="_blank">https://www.amazon.it/Google-Hacking-Penetration-Testers-Johnny/dp/0128029641</a><br />
    ◇ The Google Hacker’s Guide(Johnny Long): <a
      href="http://pdf.textfiles.com/security/googlehackers.pdf" target="_blank">http://pdf.textfiles.com/security/googlehackers.pdf</a><br />•
    <strong>Sites</strong><br /> ◇ Google Hacking database: <a
      href="https://www.exploit-db.com/google-hacking-database" target="_blank">https://www.exploit-db.com/google-hacking-database</a>
  </div>
</body>

</html>