<!doctype html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Subdomain Enumeration</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="res/styles3.css" type="text/css" />
</head>

<body>
  <div class='page'>
    <h1 class='title'>Subdomain Enumeration</h1><br />With Subdomain enumeration is possible identify resources that may
    contain contain:<br /> ◇ outdated vulnerable software<br /> ◇ sensitive data <br /> ◇ administrative interfaces that
    are not secured<br /> ◇ ...<br /><br /><strong>Comparative between subdomain enumeration tools</strong><br /><a
      href="https://twitter.com/Six2dez1" target="_blank">@Six2dez1</a> has done a good comparative between subdomain enumeration tools.
    In short subfinder is the best, followed by amass<br />
    https://six2dez.gitbook.io/pentest-book/others/subdomain-tools-review<br /><br /><strong>Check also the advise of
    </strong><strong><span style="text-decoration:underline;">Zseano</span></strong><strong> in the chapter:</strong> →
    <a
      href="Home--Penetration_Test_Methodology--WebApp_Pentest--Reconnaissance_(Information_Gathering)--Subdomain_Enumeration--METHODOLOGY.html">Zseano
      Methodology</a> ← <br /><br /><strong>What are Virtual Hosts</strong><br />A virtual host is simply a website that
    shares an IP address with one or more domains and subdomains.<br /><br /> Tools:<br /><strong>•</strong>
    <strong>dnsdumbster</strong>: <a href="https://dnsdumpster.com/" target="_blank">https://dnsdumpster.com/</a> → collects data from
    Search Engines indexing (e.g: Google, Bing, Virustotal...)<br /><strong>•</strong> <strong>sublist3r</strong>: <a
      href="https://github.com/aboul3la/Sublist3r" target="_blank">https://github.com/aboul3la/Sublist3r</a><br /> see this chapter <a
      href="Home--Penetration_Test_Methodology--WebApp_Pentest--Reconnaissance_(Information_Gathering)--Subdomain_Enumeration--sublist3r_(OSINT).html">HERE</a><br />
    <a href=""><img src="images/283-1.png" alt="images/283-1.png" /></a><br />• <strong>Amass</strong>: <br /> see this
    subchapter <a
      href="Home--Penetration_Test_Methodology--WebApp_Pentest--Reconnaissance_(Information_Gathering)--Subdomain_Enumeration--amass.html">HERE</a><br /><strong>•</strong>
    <strong>virustotal: </strong><a href="https://www.virustotal.com/">https://www.virustotal.com/</a><strong><br />
    </strong><a href=""><img src="images/283-2.png" alt="images/283-2.png" /></a><br /> <a href=""><img
        src="images/283-3.png" alt="images/283-3.png" /></a><br /> <a href=""><img src="images/283-4.png"
        alt="images/283-4.png" /></a><br />• <strong>crt.sh</strong>: <a href="https://crt.sh/">https://crt.sh/</a> →
    collect SSL certificate of the domains and subdomains<br />• <strong>nmap NSE</strong><br /> ◇ script
    “http-robots.txt”<br /> To identify data found in the robots.txt file. These directories can be interesting<br />
    This page inform us that there is a <strong>robot.txt</strong> file, <br /> A <strong>robots.txt</strong> file is
    common on web applications. This file tells to <span style="text-decoration:underline;">&quot;web crawlers”</span>,
    which scan the internet for the various web search engines to present up-to-date search results, which paths of the
    web application should not be indexed in search engine results.<br /> Because these paths are voluntarily hidden by
    developers maybe are sensitive paths that we want to check<br /><strong> “Disallow” </strong>instruction means that
    web crawlers can not include these paths of the application in the search results<br /> now we can save only the
    paths in a file<br />
    <div class="codebox">
      <div class="codebox">nmap&nbsp;--script=http-robots.txt&nbsp;<span
          style="color:#000000;font-weight:400">[</span>server<span style="color:#000000;font-weight:400">]</span></div>
    </div><br /> <a href=""><img src="images/283-5.png" alt="images/283-5.png" /></a>
  </div>
</body>

</html>